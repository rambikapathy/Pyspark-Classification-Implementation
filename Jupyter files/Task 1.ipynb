{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "025af822",
   "metadata": {},
   "source": [
    "# Big Data(Task 1)\n",
    "\n",
    "\n",
    "\n",
    "## Are there missing values? \n",
    "There are currently no missing values in the dataset.\n",
    "Having filtered null rows using isNULL() function.\n",
    "\n",
    "### Discuss how you will deal with missing values, even if there are no missing values in this data set.\n",
    "\n",
    "In order to deal with missing values, we can either utilize the replace(), fill() and drop() methods.. \n",
    "\n",
    "However within this sequence, I have implented the count for missing values using the isnan() Function. \n",
    "The Column name (c) df.column is passed to isnan() function to output missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4648ccf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pyspark\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#import spark session\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col,isnan, when, count,sum\n",
    "\n",
    "#create spark session \n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52485af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reads dataset\n",
    "df = spark.read.csv(\"nuclear_plants_small_dataset.csv\",inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4774dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Status: string (nullable = true)\n",
      " |-- Power_range_sensor_1: double (nullable = true)\n",
      " |-- Power_range_sensor_2: double (nullable = true)\n",
      " |-- Power_range_sensor_3 : double (nullable = true)\n",
      " |-- Power_range_sensor_4: double (nullable = true)\n",
      " |-- Pressure _sensor_1: double (nullable = true)\n",
      " |-- Pressure _sensor_2: double (nullable = true)\n",
      " |-- Pressure _sensor_3: double (nullable = true)\n",
      " |-- Pressure _sensor_4: double (nullable = true)\n",
      " |-- Vibration_sensor_1: double (nullable = true)\n",
      " |-- Vibration_sensor_2: double (nullable = true)\n",
      " |-- Vibration_sensor_3: double (nullable = true)\n",
      " |-- Vibration_sensor_4: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#displays content of spark dataframe\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f09a9b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------\n",
      " Status                | 0   \n",
      " Power_range_sensor_1  | 0   \n",
      " Power_range_sensor_2  | 0   \n",
      " Power_range_sensor_3  | 0   \n",
      " Power_range_sensor_4  | 0   \n",
      " Pressure _sensor_1    | 0   \n",
      " Pressure _sensor_2    | 0   \n",
      " Pressure _sensor_3    | 0   \n",
      " Pressure _sensor_4    | 0   \n",
      " Vibration_sensor_1    | 0   \n",
      " Vibration_sensor_2    | 0   \n",
      " Vibration_sensor_3    | 0   \n",
      " Vibration_sensor_4    | 0   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(*(sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns)).show(vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c01378",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find missing values within a dataframe\n",
    "\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "#df.count finds total value for rows\n",
    "rows = df.count()\n",
    "\n",
    "#df.describe returns description of dataframe \n",
    "# it can also be interpreted as returning statistical summary of dataframe \n",
    "summary = df.describe().filter(col(\"summary\") == \"count\")\n",
    "summary.select(*((lit(rows)-col(c)).alias(c) for c in df.columns)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09811ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we are finding the filtering rows with NULL values within nuclear dataset. \n",
    "#select() or where() functions of Spark dataframe performs the filtering of rows with NULL values when utilising isNULL(). \n",
    "#The statement below returns all rows that have null values on the state column.\n",
    "df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e56ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can also use .drop to dropping rows having null values\n",
    "df.na.drop().show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
